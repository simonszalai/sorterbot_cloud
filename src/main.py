"""
Main module responsible for orchestrating image processing.

"""

import os
from yaml import load, Loader, YAMLError
from pathlib import Path
from dotenv import load_dotenv

from locator.detectron import Detectron
from vectorizer.vectorizer import Vectorizer
from utils.postgres import Postgres
from utils.logger import logger
from utils.S3 import S3


class Main:
    """
    Main class for controlling image processing. When instantiated, it loads all neccessary environment
    variables and config files and instantiates all needed modules.

    Parameters
    ----------
    db_name : str
        Name of the postgres database to be created. Should be different for test and production.
    base_img_path : str
        Location where the downloaded images should be stored.

    """

    def __init__(self, db_name, base_img_path):
        # Load envirnoment vectorizer from .env in project root
        load_dotenv()

        # Parse config.yaml
        with open("config.yaml", 'r') as stream:
            try:
                config = load(stream, Loader)
            except YAMLError as error:
                logger.error("Error while opening config.yaml ", error)

        self.base_img_path = base_img_path

        self.postgres = Postgres(db_name=db_name)
        self.s3 = S3(base_img_path=self.base_img_path)
        self.detectron = Detectron(
            base_img_path=self.base_img_path,
            model_config=config["DETECTRON"]["MODEL_CONFIG"],
            threshold=config["DETECTRON"]["THRESHOLD"]
        )
        self.vectorizer = Vectorizer(
            base_img_path=self.base_img_path,
            model_name=config["VECTORIZER"]["MODEL"],
            input_dimensions=config["VECTORIZER"]["INPUT_DIMS"],
            batch_size=config["VECTORIZER"]["BATCH_SIZE"]
        )

    def process_image(self, session_id, image_name):
        """
        This method runs object recognition on the passed image and saves the result to the database.

        Parameters
        ----------
        session_id : str
            Datetime based unique identifier of the current session. It is generated by the Raspberry Pi and passed
            with the POST request.
        image_name : str
            Name of the image to be processed. The image has to be uploaded to the s3 bucket. Value is passed
            with the POST request.

        """

        # Open postgres connection and create table for current session if it does not exist yet
        self.postgres.open()
        self.postgres.create_table(table_name=session_id)

        # Create folders for original and cropped images if they do not exist
        Path(os.path.join(self.base_img_path, session_id, "original")).mkdir(parents=True, exist_ok=True)
        Path(os.path.join(self.base_img_path, session_id, "cropped")).mkdir(parents=True, exist_ok=True)

        # Download image if needed
        self.s3.download_image(session_id, image_name)
        # Run detectron to get bounding boxes
        results = self.detectron.predict(session_id=session_id, image_name=image_name)

        # Insert bounding box locations to postgres
        self.postgres.insert_results(results)

    def vectorize_session_images(self, session_id):
        """
        This method is to be executed after the last image of a session is processed. It gets a list of unique
        images in the current session, retrieves all the objects that belong to each image and runs the vectorizer
        on them.

        Returns
        -------
        pairings : list
            List of dicts containing pairs of objects and clusters.
        session_id : str
            Datetime based unique identifier of the current session. It is generated by the Raspberry Pi and passed
            with the POST request.

        """

        self.postgres.open()

        # Get list of unique image names in current session
        unique_images = self.postgres.get_unique_images()

        # Get objects belonging to each unique image from postgres
        # The nested structure here improves efficiency when images are cropped (no need to open and close an image multiple times)
        images_with_objects = []
        for image_name in unique_images:
            objects_of_img = self.postgres.get_objects_of_image(image_name=image_name)
            images_with_objects.append({
                "image_name": image_name,
                "objects": objects_of_img
            })

        def get_bbox_center(obj):
            x = int((obj["bbox_dims"]["x1"] + obj["bbox_dims"]["x2"]) / 2)
            y = int((obj["bbox_dims"]["y1"] + obj["bbox_dims"]["y2"]) / 2)
            return (x, y,)

        # Create separate lists of items and containers
        items = []
        containers = []
        for img in images_with_objects:
            for obj in img["objects"]:
                (items if obj["class"] == 0 else containers).append({
                    "img_base_angle": int(Path(img["image_name"]).stem),
                    "img_dims": obj["img_dims"],
                    "obj_id": obj["id"],
                    "bbox_center_point": get_bbox_center(obj)
                })

        self.postgres.close()

        # Handle case if no containers were found
        n_containers = len(containers)
        if n_containers == 0:
            raise Exception("No containers were found!")

        # Run vectorizer to assign each object to a cluster
        pairings = self.vectorizer.run(session_id=session_id, images=images_with_objects, n_containers=n_containers)

        def pairing_matches_item(pairing, item):
            return pairing["image_id"] == item["img_base_angle"] and pairing["obj_id"] == item["obj_id"]

        commands = []
        for item in items:
            target_cont = next(containers[pairing["cluster"]] for pairing in pairings if pairing_matches_item(pairing, item))
            commands.append({
                "img_base_angle": item["img_base_angle"],
                "img_dims": item["img_dims"],
                "command": (item["bbox_center_point"], target_cont["bbox_center_point"])
            })

        return commands
